{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U cragmm-search-pipeline\n",
    "%pip install datasets torch transformers matplotlib accelerate ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom cragmm_search.search import UnifiedSearchPipeline\\n\\nsearch_pipeline = UnifiedSearchPipeline(\\n    image_model_name=\"openai/clip-vit-large-patch14-336\",\\n    image_hf_dataset_id=\"crag-mm-2025/image-search-index-validation\",\\n    text_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\\n    web_hf_dataset_id=\"crag-mm-2025/web-search-index-validation\",\\n)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from cragmm_search.search import UnifiedSearchPipeline\n",
    "\n",
    "search_pipeline = UnifiedSearchPipeline(\n",
    "    image_model_name=\"openai/clip-vit-large-patch14-336\",\n",
    "    image_hf_dataset_id=\"crag-mm-2025/image-search-index-validation\",\n",
    "    text_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    web_hf_dataset_id=\"crag-mm-2025/web-search-index-validation\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration, MllamaConfig\n",
    "from transformers.utils import ModelOutput\n",
    "from torch import nn\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithPastAndBBox(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: Optional[torch.FloatTensor] = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor, ...], ...]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    bbox_coords: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class BoundingBoxHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # [x1, y1, x2, y2] pixel relative coordinates (0â€“1) to input image size.\n",
    "        self.bbox_regression_head = nn.Linear(4096, 4)\n",
    "        self.dtype = self.bbox_regression_head.weight.dtype\n",
    "\n",
    "    def forward(self, attention_mask, last_hidden_states, bbox_binary_label=None, bbox_coords_label=None):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).to(self.dtype)\n",
    "        sub_embeddings = torch.sum(last_hidden_states * attention_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(attention_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled_output = sub_embeddings / sum_mask  # (batch_size, hidden_dim)\n",
    "\n",
    "        bbox_coords = self.bbox_regression_head(pooled_output)  # (batch_size, 4)\n",
    "\n",
    "        loss = None\n",
    "        if bbox_binary_label is not None:\n",
    "            bbox_coord_loss = nn.MSELoss()(bbox_coords, bbox_coords_label)\n",
    "            loss = bbox_coord_loss\n",
    "\n",
    "        return {\"loss\": loss, \"bbox_coords\": bbox_coords}\n",
    "\n",
    "\n",
    "class CustomLlamaVLM(MllamaForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bbox = BoundingBoxHead()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        return_dict=None,\n",
    "        request_bounding_box=False,\n",
    "        bbox_binary_label=None,\n",
    "        bbox_coords_label=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "\n",
    "        # Call the parent `forward` method\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True,  # required for bounding box\n",
    "            return_dict=return_dict,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        loss = None\n",
    "        bbox_coords = None\n",
    "        if request_bounding_box:\n",
    "            bbox = self.bbox(attention_mask, outputs.hidden_states[-1], bbox_binary_label=bbox_binary_label, bbox_coords_label=bbox_coords_label)\n",
    "            bbox_coords = bbox[\"bbox_coords\"]\n",
    "            loss = bbox[\"loss\"]\n",
    "\n",
    "        return CausalLMOutputWithPastAndBBox(\n",
    "            loss=loss,\n",
    "            logits=outputs.logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            bbox_coords=bbox_coords\n",
    "        )\n",
    "\n",
    "# Usage example\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# Initialize components\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = CustomLlamaVLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant that can answer questions about images. You will be given an image and a question. If the question requires a bounding box, output the bounding box in the format of [x1, y1, x2, y2]. If the question does not require a bounding box, output \\\"no\\\". Do not output any other additional text.\"\n",
    "prompt_template = \"Given this question \\\"{question}\\\", output a single bounding box in the format of [x1, y1, x2, y2] that best captures the object to answer this question.\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import patches\n",
    "from datasets import load_dataset\n",
    "\n",
    "def draw_image_with_bbox(image, bbox_coords):\n",
    "    _, ax = plt.subplots()\n",
    "    plt.imshow(image)\n",
    "    left, top, right, bottom = bbox_coords\n",
    "    width, height = image.size\n",
    "    box_width = (right - left) * width\n",
    "    box_height = (top - bottom) * height\n",
    "    rect = patches.Rectangle((left * width, bottom * height), box_width, box_height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "cache_dir = os.path.join(home_dir, \"image_cache\")\n",
    "\n",
    "def preload_and_cache(dataset, cache_dir=cache_dir, num_workers=4):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    def download_sample(sample):\n",
    "        url = sample[\"image\"]\n",
    "        filename = hashlib.md5(url.encode()).hexdigest() + \".jpg\"\n",
    "        filepath = os.path.join(cache_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            return  # Already cached\n",
    "        try:\n",
    "            response = requests.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            image.save(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        list(tqdm(executor.map(download_sample, dataset), total=len(dataset)))\n",
    "\n",
    "class LazyDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, system_prompt, user_prompt_template, transform=None, cache_dir=cache_dir):\n",
    "        self.samples = dataset\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt_template = user_prompt_template\n",
    "        self.processor = processor\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        img = self._load_image(sample)\n",
    "        bbox_coords = self._process_bbox(sample)\n",
    "        texts = self._process_text(sample)\n",
    "        need_bboxs = torch.ones(1)\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"texts\": texts,\n",
    "            \"need_bboxs\": need_bboxs,\n",
    "            \"bbox_coords\": bbox_coords,\n",
    "        }\n",
    "    \n",
    "    def _load_image(self, sample):\n",
    "        url = sample[\"image\"]\n",
    "        filename = hashlib.md5(url.encode()).hexdigest() + \".jpg\"\n",
    "        filepath = os.path.join(self.cache_dir, filename)\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(filepath):\n",
    "                img = Image.open(filepath).convert(\"RGB\")\n",
    "            else:\n",
    "                response = requests.get(url, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                img.save(filepath)\n",
    "            img = img.resize((640, 480))\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image from {url}: {e}\")\n",
    "\n",
    "    def _process_bbox(self, sample):\n",
    "        return torch.tensor([sample[\"left\"] / sample[\"width\"], sample[\"top\"] / sample[\"height\"], sample[\"right\"] / sample[\"width\"], sample[\"bottom\"] / sample[\"height\"]])\n",
    "\n",
    "    def _process_text(self, sample):\n",
    "        return self.processor.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": self.user_prompt_template.format(question=sample[\"question\"])},\n",
    "                ]},\n",
    "            ],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "\n",
    "# Load the dataset split\n",
    "hf_dataset = load_dataset(\"toloka/WSDMCup2023\", split=\"train\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = LazyDataset(hf_dataset, processor, system_prompt, prompt_template)\n",
    "preload_and_cache(dataset, cache_dir=cache_dir)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=12, shuffle=True, num_workers=2, multiprocessing_context=\"fork\", prefetch_factor=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.bbox.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "model.train()\n",
    "running_loss = 0\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    num_batches = 0\n",
    "    batch_bar = tqdm(dataloader)\n",
    "    for batch in batch_bar:\n",
    "        num_batches += 1\n",
    "\n",
    "        inputs = processor(\n",
    "            text=batch[\"texts\"],  # List of str\n",
    "            images=[[img] for img in batch[\"image\"]],  # must break a batch of image into a list\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "            padding=True, # needed for batch\n",
    "            truncation=True # needed for batch\n",
    "        ).to(model.device)\n",
    "\n",
    "        bbox_binary_label = batch[\"need_bboxes\"].to(model.device, dtype=model.dtype)\n",
    "        bbox_coords_label = batch[\"bbox_coords\"].to(model.device, dtype=model.dtype)\n",
    "        outputs = model(**inputs, request_bounding_box=True, bbox_binary_label=bbox_binary_label, bbox_coords_label=bbox_coords_label)\n",
    "        loss = outputs[\"loss\"]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Plot every 10 batches\n",
    "        if (num_batches + 1) % 10 == 0:\n",
    "            avg_loss = running_loss / 10\n",
    "            loss_values.append(avg_loss)\n",
    "            print(f\"[Epoch {epoch+1}], [Batch {num_batches + 1}], [Loss: {avg_loss:.4f}]\")\n",
    "            running_loss = 0.0\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.title('Training Loss')\n",
    "            plt.xlabel('Every 10 Batches')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.plot(loss_values, label='Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            display(batch_bar.container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "single_turn_dataset = load_dataset(\"crag-mm-2025/crag-mm-single-turn-public\", revision=\"v0.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for index in range(10):\n",
    "    image = single_turn_dataset[\"validation\"][index][\"image\"]\n",
    "    resize_image = image.resize((640, 480))\n",
    "    question = single_turn_dataset[\"validation\"][index][\"turns\"][0][\"query\"]\n",
    "    ground_truth = single_turn_dataset[\"validation\"][index][\"answers\"][0][\"ans_full\"]\n",
    "\n",
    "    prompt = question\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt_template.format(question=question)},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=[resize_image],\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        ).to(model.device)\n",
    "\n",
    "        output = model(**inputs, request_bounding_box=True)\n",
    "\n",
    "        draw_image_with_bbox(resize_image, output.bbox_coords[0].cpu().float())\n",
    "\n",
    "        output = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "        print(ground_truth)\n",
    "        out = processor.decode(output[0], skip_special_tokens=True).split(\"assistant\\n\", 1)[1]\n",
    "        text_bbox = eval(out)\n",
    "        draw_image_with_bbox(resize_image, text_bbox)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
